# ğŸš€ AI-Powered Performance Degradation Analyzer

This project provides an interactive dashboard for running, logging, visualizing, and AI-analyzing load test results for web endpoints. It is designed to help you detect, compare, and understand performance regressions in your web services using Locust and an LLM-powered analysis tool.

## âœ¨ Features

- ğŸƒ **Run Load Tests:** Easily configure and execute load tests on any HTTP endpoint using Locust.
- ğŸ“º **Live Log Streaming:** View real-time logs and progress of running tests in the dashboard.
- ğŸ’¾ **Result Storage:** Each test run is saved as a JSON file for later comparison.
- ğŸ“Š **Visual Comparison:** Select and compare two runs with clear bar charts of key metrics (latency, error rate, requests/sec, etc.).
- ğŸ¤– **AI Analysis:** Get a natural language summary and risk assessment (Stable, âš ï¸ Warning, ğŸ”¥ Threat, âœ… Conclusion) of performance differences using an LLM (Claude or similar) via a custom MCP server.
- ğŸ•‘ **History Management:** Clear or refresh run history from the sidebar.

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py              # Streamlit dashboard UI and logic
â”‚   â”œâ”€â”€ mcp_runner.py       # Helper to call the Claude MCP server for analysis
â”œâ”€â”€ locust_tests/
â”‚   â””â”€â”€ test_scenario.py    # Locust test scenario (dynamic endpoint)
â”œâ”€â”€ mcp_server/
â”‚   â”œâ”€â”€ claude_perf_mcp.py  # JSON-RPC server for LLM-based analysis
â”‚   â””â”€â”€ mcp/                # Minimal MCP server framework
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ run_logger.py       # Utility for saving run data
â”œâ”€â”€ data/
â”‚   â””â”€â”€ runs/               # Stores all run result JSON files
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .gitignore              # Git ignore rules
â”œâ”€â”€ README.md               # Project documentation
```

## âš¡ Quick Start

1. ğŸ› ï¸ **Install dependencies:**
   ```powershell
   pip install -r requirements.txt
   ```

2. ğŸ–¥ï¸ **Run the dashboard:**
   ```powershell
   streamlit run dashboard/app.py
   ```

3. ğŸ—ï¸ **Run a test:**
   - Enter the Base URL and Endpoint.
   - Set users, spawn rate, and duration.
   - Click "Run Test" and watch live logs and progress.

4. ğŸ“ˆ **Compare runs:**
   - Select two runs from the dropdowns.
   - Click "Compare Performance" for a chart.
   - Click "Claude Analysis" for an AI-powered summary and risk assessment.

## ğŸ§  How It Works

- ğŸ **Locust** runs the load test and writes results to `data/runs/` as JSON.
- ğŸ›ï¸ **Streamlit dashboard** lets you run tests, view logs, and compare results.
- ğŸ¤– **Claude MCP server** (Python, JSON-RPC) compares two runs and returns a natural language summary with risk/conclusion labels.

## ğŸ› ï¸ Customization

- âœï¸ To change the test scenario, edit `locust_tests/test_scenario.py`.
- ğŸ§© To improve AI analysis, edit `mcp_server/claude_perf_mcp.py`.
- ğŸ“ To add new metrics, update both the Locust scenario and the dashboard comparison logic.

## ğŸ“‹ Requirements
- Python 3.8+
- See `requirements.txt` for all dependencies

## ğŸ“„ License
MIT License

---

ğŸ‰ **Enjoy fast, AI-powered performance analysis for your APIs!** ğŸš¦
